{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python_defaultSpec_1596758896729","display_name":"Python 3.6.10 64-bit ('igcmf': conda)"},"colab":{"name":"IGCMF.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"..\")"]},{"cell_type":"code","metadata":{"id":"OoO4o8X9DhVs","colab_type":"code","colab":{}},"source":["\n","import numpy as np\n","import torch\n","import pandas as pd\n","import numpy as np\n","import os\n","#\n","import pickle as pkl\n","#\n","from sklearn import preprocessing\n","#\n","import matplotlib\n","matplotlib.use('agg')"],"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"PyTorch version: 1.5.0\nPyTorch CUDA version: 10.1\nis CUDA available? True\n"}],"source":["print('PyTorch version:', torch.__version__) # v1.5.0\n","print(\"PyTorch CUDA version:\",torch.version.cuda) # v10.2\n","torch.cuda.current_device()\n","print('is CUDA available?',torch.cuda.is_available())"]},{"cell_type":"code","metadata":{"id":"IPN32L8SESFZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"status":"error","timestamp":1596604696557,"user_tz":-600,"elapsed":884,"user":{"displayName":"Riordan Alfredo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLt9xRjtNZnB33NvOyHKPHq5AVG7hsg4zHVdcDCg=s64","userId":"12730100241040769811"}},"outputId":"aec93b43-247f-4474-c42a-22bd0dc752dc"},"source":["from torch_geometric.nn import GCNConv, RGCNConv, global_sort_pool, global_add_pool\n","from torch_geometric.utils import dropout_adj"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zxoIiwpuZdpC"},"source":["# Overall Architecture\n","\n","0. Arrange multi-view matrices (prepare matrices)\n","1. Select closely related matrices (matrices that share common entity)\n","2. Generate subgraphs (multi-partite graphs) for all matrix groups\n","3. Node labeling \n","4. Transform a multi-partite graph to bipartite graphs as layers\n","5. Group up similar bipartite-graphs for training (inspired by IGMC)\n","6. Training with GNN using multiple layers (potentially using R-GCN) concurrently\n","\n","**Loss function**: Reduce MSE with random bias for each layers (need to revisit GC-MC and R-GCN)\n","\n","**Optimization**: Adam optimizer\n","\n","**Evaluation with other models**: RMSE and AUC\n"]},{"cell_type":"code","metadata":{"id":"fi5SHuWeESFd","colab_type":"code","colab":{}},"source":["# initialise all necesarry functions here. Maybe should be moved to another file and import later\n","def one_hot(idx, length):\n","    idx = np.array(idx)\n","    x = np.zeros([len(idx), length])\n","    x[np.arange(len(idx)), idx] = 1.0\n","    return x"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cz7L7PvkESFg","colab_type":"text"},"source":["# Step 0:  Prepare matrices\n","This includes:\n","* Load matrices \n","* Arrange matrices"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["data_dir = '../../data/sample_data/'\n","num_folds = 1"]},{"cell_type":"markdown","metadata":{},"source":["## 1.Load data\n","Load matrices. Data is taken from dCMF paper"]},{"cell_type":"code","execution_count":6,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"== Loading data from data_dir:  ../../data/sample_data/\nLoaded! dict_keys(['U1', 'U2', 'V1', 'W1', 'R'])\n"}],"source":["# load matrices\n","print(\"== Loading data from data_dir: \", data_dir)\n","U1 = pkl.load(open(data_dir+\"X_13.pkl\", 'rb'))\n","U2 = pkl.load(open(data_dir+\"X_14.pkl\", 'rb'))\n","V1 = pkl.load(open(data_dir+\"X_26.pkl\", 'rb'))\n","W1 = pkl.load(open(data_dir+\"X_53.pkl\", 'rb'))\n","r_temp_dict = {}\n","for fold_num in np.arange(1, num_folds+1):\n","    r_train = pkl.load(open(data_dir+'/X_12_train_fold_'+str(fold_num)+'.pkl', 'rb'))\n","    r_train_idx = pkl.load(open(data_dir+'/X_12_train_idx_'+str(fold_num)+'.pkl', 'rb'))\n","    r_test = pkl.load(open(data_dir+'/X_12_test_fold_'+str(fold_num)+'.pkl', 'rb'))\n","    r_test_idx = pkl.load(open(data_dir+'/X_12_test_idx_'+str(fold_num)+'.pkl', 'rb'))\n","    r_doublets = pkl.load(open(data_dir+'/R_doublets_'+str(fold_num)+'.pkl', 'rb'))\n","    r_temp_dict[fold_num] = {\"Rtrain\": r_train, \"Rtrain_idx\": r_train_idx, \"Rtest\": r_test, \"Rtest_idx\": r_test_idx, \"Rdoublets\": r_doublets}\n","\n","data_dict = {\"U1\": U1, \"U2\": U2, \"V1\": V1, \"W1\": W1, \"R\": r_temp_dict}\n","print(\"Loaded!\", data_dict.keys())"]},{"cell_type":"code","execution_count":7,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"U1.shape:  (1000, 20)\nU2.shape:  (1000, 150)\nV1.shape:  (2000, 250)\nW1.shape:  (300, 20)\nR.shape:  (1000, 2000)\n"}],"source":["print(\"U1.shape: \",U1.shape)\n","print(\"U2.shape: \",U2.shape)\n","print(\"V1.shape: \",V1.shape)\n","print(\"W1.shape: \",W1.shape)\n","print(\"R.shape: \",data_dict['R'][1]['Rtrain'].shape)\n","# print(\"U1 data:\",U1)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Preprocess matrices\n","> TODO: create training, validation, and test data for each matrices by dropping out matrices randomly \n","\n","Inspired by IGMC, load matrix using a method from Monti et al."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import src.preprocessing # custom preprocessing functions"]},{"cell_type":"code","execution_count":14,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[0., 1., 0.,  ..., 1., 0., 0.],\n        [1., 0., 0.,  ..., 1., 1., 0.],\n        [1., 1., 0.,  ..., 0., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 0., 1.],\n        [0., 1., 1.,  ..., 0., 0., 1.],\n        [0., 0., 0.,  ..., 1., 1., 1.]])\ntensor([[   0,    1],\n        [   0,    6],\n        [   0,    9],\n        ...,\n        [ 999, 1997],\n        [ 999, 1998],\n        [ 999, 1999]])\n[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.8057179  0.         0.         ... 0.83843999 0.75367971 0.        ]\n ...\n [0.81335164 0.         0.         ... 0.87319588 0.76432996 0.78645457]\n [0.82397774 0.         0.         ... 0.83491009 0.80835884 0.86646459]\n [0.9348797  0.         0.         ... 0.90277274 0.83821403 0.90299402]]\n"},{"output_type":"execute_result","data":{"text/plain":"tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.8057, 0.0000, 0.0000,  ..., 0.0000, 0.7537, 0.0000],\n        ...,\n        [0.8134, 0.0000, 0.0000,  ..., 0.8732, 0.0000, 0.7865],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8665],\n        [0.0000, 0.0000, 0.0000,  ..., 0.9028, 0.8382, 0.9030]],\n       dtype=torch.float64)"},"metadata":{},"execution_count":14}],"source":["# TODO: work on this function to split matrix between testing, validation and test\n","# after that, move it to src.preprocessing file!\n","def splitting_data(matrix):\n","    m_torch = torch.from_numpy(matrix)\n","    holes = torch.empty(m_torch.shape).random_(2)\n","    holes_idx = torch.nonzero(holes)\n","    training_data = torch.mul(m_torch,holes) # one-hot encoding to give holes on matrices\n","    training_idx = torch.nonzero(training_data)\n","    print(holes)\n","    print(holes_idx)\n","    return (matrix,training_data, None, None)\n","    \n","matrix = data_dict['R'][1]['Rtrain']\n","m, train_data, test_data, val_data =  splitting_data(matrix)\n","print(m)\n","train_data"]},{"cell_type":"code","execution_count":17,"metadata":{"tags":[]},"outputs":[],"source":["# TODO\n","\n","\"\"\"\n","function to extract labels and indices from a matrix.\n","Taken from Monti et al. with slight modifications (remove training,validation, and test)\n","\"\"\"\n","def extract_matrix_info(M, testing=False):\n","    num_users = M.shape[0]\n","    num_items = M.shape[1]\n","\n","    u_nodes_ratings = np.where(M)[0]\n","    v_nodes_ratings = np.where(M)[1]\n","    ratings = M[np.where(M)]\n","\n","    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(\n","        np.int64), v_nodes_ratings.astype(np.int32)\n","    ratings = ratings.astype(np.float64)\n","\n","    u_nodes = u_nodes_ratings\n","    v_nodes = v_nodes_ratings\n","\n","    print('number of users = ', len(set(u_nodes)))\n","    print('number of item = ', len(set(v_nodes)))\n","\n","    # assumes that ratings_train contains at least one example of every rating type\n","    rating_dict = {r: i for i, r in enumerate(\n","        np.sort(np.unique(ratings)).tolist())}\n","\n","    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n","    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n","\n","\n","    for i in range(len(u_nodes)):\n","        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n","\n","    labels = labels.reshape([-1])\n","\n","    # number training\n","    num_train = np.where(Otraining)[0].shape[0]\n","    num_val = int(np.ceil(num_train * 0.2))\n","    num_train = num_train - num_val\n","\n","    # non_zero arrays\n","    pairs_nonzero_train = torch.nonzero(torch.from_numpy(M))\n","    idx_nonzero_train = np.array(\n","        [u * num_items + v for u, v in pairs_nonzero_train])\n","\n","     # Internally shuffle training set (before splitting off validation set)\n","    rand_idx = list(range(len(idx_nonzero_train)))\n","    np.random.seed(42)\n","    np.random.shuffle(rand_idx)\n","    idx_nonzero_train = idx_nonzero_train[rand_idx]\n","    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n","\n","    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n","    pairs_nonzero = np.concatenate(\n","        [pairs_nonzero_train, pairs_nonzero_test], axis=0)\n","\n","    train_idx = idx_nonzero[num_val:num_train + num_val]\n","    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n","    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n","\n","    train_labels = labels[train_idx]\n","\n","    class_values = np.sort(np.unique(ratings))\n","\n","    '''Note here rating matrix elements' values + 1 !!!'''\n","\n","    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n","\n","    if post_rating_map is None:\n","        rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n","    else:\n","        rating_mx_train[train_idx] = np.array(\n","            [post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n","\n","    rating_mx_train = sp.csr_matrix(\n","        rating_mx_train.reshape(num_users, num_items))\n","\n","    if u_features is not None:\n","        u_features = sp.csr_matrix(u_features)\n","        print(\"User features shape: \" + str(u_features.shape))\n","\n","    if v_features is not None:\n","        v_features = sp.csr_matrix(v_features)\n","        print(\"Item features shape: \" + str(v_features.shape))\n","\n","    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, class_values"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["G = {\n","    \"e1\":[\"X1\",\"X2\",\"X3\"],\\\n","    \"e2\":[\"X1\",\"X4\"],\\\n","    \"e3\":[\"X2\",\"X5\"],\\\n","    \"e4\":[\"X3\"],\\\n","    \"e5\":[\"X5\"],\\\n","    \"e6\":[\"X4\"]}"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["X_data = {\n","    \"X1\":{\"1\":data_dict['R'][1][\"Rtrain\"]},\\\n","    \"X2\":{\"1\":U1},\\\n","    \"X3\":U2,\\\n","    \"X4\":V1,\\\n","    \"X5\":W1}"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["X_meta = {\n","    \"X1\":[\"e1\",\"e2\"],\\\n","    \"X2\":[\"e1\",\"e3\"],\\\n","    \"X3\":[\"e1\",\"e4\"],\\\n","    \"X4\":[\"e2\",\"e6\"],\\\n","    \"X5\":[\"e5\",\"e3\"]}"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["Rtest_triplets1 = [[1,1,1],[2,2,0]]\n","Rtest_triplets2 = [[1,1,1],[3,3,0],[1,2,0],[0,1,0],[0,2,0],[0,3,0]]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["X_val = {\n","    \"X1\":{\"1\":Rtest_triplets1},\n","    \"X2\":{\"1\":Rtest_triplets2}\n","}"]},{"cell_type":"markdown","metadata":{"id":"fREOlaeVqp6C","colab_type":"text"},"source":["# Step 1: Select sub-matrix that shares common entities\n","\n","After arranging matrices, we will \n","* Select the size of sub-matrix *(m x n)* where 'm' is the size of common domain (i.e. user)"]},{"cell_type":"code","metadata":{"id":"J6ZFsjMsESFh","colab_type":"code","colab":{}},"source":["\n","\n","# Question: How to extract data from matrix and build it into subgraphs? Using torch_geometric! see documentation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BpNRWf0Qhzwu","colab_type":"text"},"source":["# Step 2: Generate subgraphs for all matrix groups\n","\n","Do the following until cover all groups\n","* In one group, by using sub-matrix size from first step, loop connected matrices to generate multi-partite graphs.\n","* Please refer to step 3 for node-labelling process."]},{"cell_type":"markdown","metadata":{"id":"LgTrnc7zESFj","colab_type":"text"},"source":["# Step 3: Node labelling\n","\n","From arranged matrices, label them from 0 to N to build multipartite graph. For each entities, the label goes from N+1 to N+N, in the first hop. The following hops goes 2N+1 to 3N. So, the general formula for this node labelling is:\n","\n","> i x (N + k), \n","\n","where *i* is the number of hop, *N* is the number of involved matrices,*k* is the index of selected and arranged matrices\n","**bold text**\n"]},{"cell_type":"code","metadata":{"id":"tVrvfQnbESFk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1596160987311,"user_tz":-600,"elapsed":1038,"user":{"displayName":"Riordan Alfredo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLt9xRjtNZnB33NvOyHKPHq5AVG7hsg4zHVdcDCg=s64","userId":"12730100241040769811"}},"outputId":"687c24cf-46e2-43d7-a42d-469ce5a6a54d"},"source":["# below is just a sample model\n","# Prepare dummy variables \n","global_ids = np.array([0,3,3,3,3,3,1,4,4,4,4,4,2,5,5,5,5]) # assume 3 matrices: 0 = user; 1 = item; 2 = description\n","ids = torch.from_numpy(global_ids)\n","\n","# encode it\n","global_encoded = torch.from_numpy(one_hot(global_ids, max(global_ids)+1))\n","global_encoded"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 1., 0., 0.],\n","        [0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 1., 0.],\n","        [0., 0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 1.]], dtype=torch.float64)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"NcW8lOQXESFq","colab_type":"code","colab":{},"outputId":"7000f81a-77b4-485d-a4e8-0c3013b20831"},"source":["# parent = [target parent ids]\n","# children = [target child index, relationship weight]\n","\n","# user\n","user_parent = [[2,2],[2,2],[],[]]\n","user_children = [\\\n","                 [[0,4],[1,3]],\\\n","                 [[2,1],[3,4]],\\\n","                 [],\\\n","                 []\\\n","                ]\n","# item\n","item_parent = [[0,4],[0],[0,4],[0]]\n","item_children =[\\\n","                [[0,4],[0,1]],\\\n","                [[1,3]],\\\n","                [[2,1],[2,1]],\\\n","                [[3,4]]\\\n","               ]\n","\n","# feature\n","feature_parent = [[2],[2],[],[]]\n","feature_children = [[[0,1]],[[2,1]],[],[]]\n","\n","data = [user_parent, user_children, item_parent, item_children, feature_parent, feature_children]\n","data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[[2, 2], [2, 2], [], []],\n"," [[[0, 4], [1, 3]], [[2, 1], [3, 4]], [], []],\n"," [[0, 4], [0], [0, 4], [0]],\n"," [[[0, 4], [0, 1]], [[1, 3]], [[2, 1], [2, 1]], [[3, 4]]],\n"," [[2], [2], [], []],\n"," [[[0, 1]], [[2, 1]], [], []]]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"5ELyLedOESFt","colab_type":"code","colab":{}},"source":["# Question: How to merge similar relationship to make it undirected?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RSyGaMqGESFw","colab_type":"text"},"source":["## Node labeling functions"]},{"cell_type":"code","metadata":{"tags":[],"id":"ZSavmK3sESFw","colab_type":"code","colab":{}},"source":["def get_parent_id(data, parent_id):\n","    all_parents_pos = [id for (id,item) in enumerate(data) if item % 2 == 0]    \n","    print('function', all_parents_pos)\n","    local_parent_id = int(parent_id/2) # give index 0,1,2 etc. from parent id 2,4,6 etc.\n","    parent_idx = all_parents_pos[local_parent_id]\n","    return parent_idx\n","\n","def get_relationships(idx):\n","    # variables\n","    # idx = index\n","    # pos = position\n","\n","    c_idx = idx\n","    target_parents = []\n","    target_children = []\n","\n","    all_parents_pos = [id for (id,item) in enumerate(global_ids) if item % 2 == 0]    \n","\n","    # calculate positions\n","    c_pos = global_ids[idx] # child position\n","    p_id = c_pos if(c_idx in all_parents_pos) else c_pos - 1\n","    local_parent_id = int(p_id/2) # give index 0,1,2 etc. from parent id 2,4,6 etc.\n","    p_idx = all_parents_pos[local_parent_id]\n","    cp_pos = c_idx - p_idx - 1\n","\n","    # return\n","    target_parents = data[p_idx][cp_pos] if(not p_idx == c_idx ) else data[p_id]\n","    target_children = data[c_pos][cp_pos] if(not p_idx == c_idx ) else data[p_id+1]\n","\n","    return [target_parents,target_children]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"hAkwAwc-ESF1","colab_type":"code","colab":{},"outputId":"a9ce325b-8059-4ce4-8f9d-99c6ba5a45f1"},"source":["# Test\n","print('Node labels', global_ids)\n","\n","index = 7\n","relationships = get_relationships(index)\n","print('index:', index)\n","print('target parent:', relationships[0])\n","print('target weights:', relationships[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Node labels [0, 1, 1, 1, 1, 2, 3, 3, 3, 3, 4, 5, 5, 5, 5]\n","index: 7\n","target parent: [[2, 1]]\n","target weights: [[1, 3]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"unTH4Jmlp3F6","colab_type":"text"},"source":["# Step 4: Transform multipartite-graph into bipartite graphs"]},{"cell_type":"markdown","metadata":{"id":"4TvYCSJMESF3","colab_type":"text"},"source":["# Step 5: Group up similar bipartite-graphs for training"]},{"cell_type":"code","metadata":{"id":"H80YRxFjESF4","colab_type":"code","colab":{}},"source":["# Question: How to convert it to PyTorch, for training later?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MtCPOOstESF6","colab_type":"text"},"source":["# Step 6: Train in GNN\n","\n","Use R-GCN with 'concat' + MLP\n","\n","Activation function: either Swish/ReLU by default\n","\n","Optimized using Adam optimizer (default)\n","\n","Reduce 'MSE' with customised loss function by considering layers (need a research paper about this)"]},{"cell_type":"code","metadata":{"id":"fIXZY0hWiYTK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8l0eiUM5izSC","colab_type":"text"},"source":["# Questions/Problems\n","* How to deal the ranking problems? It happens in IGMC\n","* What will happen when we update a matrix?\n","* Can it be transferable for other dataset? It has to be inductive!"]},{"cell_type":"code","metadata":{"id":"JO1fDZvui2ag","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}